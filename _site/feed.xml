<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rob&#39;s Notes</title>
    <description>Welcome to my site. </description>
    <link>http://robinsonraju.github.io/</link>
    <atom:link href="http://robinsonraju.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 18 Dec 2016 17:52:20 -0800</pubDate>
    <lastBuildDate>Sun, 18 Dec 2016 17:52:20 -0800</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>A Simple MapReduce Job for Iris Dataset</title>
        <description>&lt;p&gt;This blog is an attempt to walk through the process of using hadoop to analyze a dataset that might look similar to an actual dataset that we might encounter.&lt;/p&gt;

&lt;p&gt;The objective is to read data from Iris dataset - &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Iris&quot;&gt;https://archive.ics.uci.edu/ml/datasets/Iris&lt;/a&gt; and calculate the mean sepal length of each class of the flower.&lt;/p&gt;

&lt;p&gt;The data is here -&amp;gt; &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Sample record : &lt;code&gt;5.1,3.5,1.4,0.2,Iris-setosa&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Attribute Information:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;sepal length in cm&lt;/li&gt;
  &lt;li&gt;sepal width in cm&lt;/li&gt;
  &lt;li&gt;petal length in cm&lt;/li&gt;
  &lt;li&gt;petal width in cm&lt;/li&gt;
  &lt;li&gt;class: a) Iris Setosa, b) Iris Versicolour, c) Iris Virginica&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the last blog on wordcount, I used an eclipse project that was bundled automatically with the Cloudera VM and just modified the StubMapper, StubReducer and Driver to plugin the code for wordcount. In this, I thought I’ll create one from scratch and see how it works.&lt;/p&gt;

&lt;h2 id=&quot;step-1--create-a-java-project&quot;&gt;Step #1 : Create a Java project&lt;/h2&gt;
&lt;p&gt;Create a Java project using Eclipse. 
Add Mapper, Reducer and Driver stubs from the other project. You’ll immediately notice that there are many compilation errors.&lt;/p&gt;

&lt;h2 id=&quot;step-2-configure-build-path&quot;&gt;Step 2: Configure Build Path&lt;/h2&gt;
&lt;p&gt;Add the following libraries from &lt;code&gt;/usr/lib/hadoop/client&lt;/code&gt; and &lt;code&gt;/cloudera/lib&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Properties &amp;gt; Java Build Path &amp;gt; Add External Jars&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/hadoop/add_lib2.png&quot; width=&quot;420&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/hadoop/add_lib1.png&quot; width=&quot;420&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Also add &lt;code&gt;&amp;lt;project&amp;gt;/conf&lt;/code&gt; as the class folder.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/hadoop/class_folder.png&quot; width=&quot;420&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now the compilation issues would be resolved.&lt;/p&gt;

&lt;h2 id=&quot;step-3-add-code-for-mapper-and-reducer&quot;&gt;Step 3: Add code for Mapper and Reducer&lt;/h2&gt;

&lt;p&gt;The objective is to read the records from the data and compute the mean sepal length of each class of the Iris flower. 
map() function in the Mapper gets each row and splits the columns to extract out the key (flower class) and the value (sepal length). 
The reduce() function is the Reducer is simple as well. It iterates through the values for each key, computes sum, maintains count and computes mean after the loop ends.&lt;/p&gt;

&lt;h3 id=&quot;mapper&quot;&gt;Mapper&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;package com.iris.data.mapreduce;
import java.io.IOException;

import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class IrisMapper extends Mapper&amp;lt;LongWritable, Text, Text, FloatWritable&amp;gt; {

	@Override
	public void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {

		String[] columns = value.toString().split(&quot;,&quot;);
		float sepal_len = Float.valueOf(columns[0]);
		String flower_class = columns[4];
		
		context.write(new Text(flower_class), new FloatWritable(sepal_len));
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;reducer&quot;&gt;Reducer&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;package com.iris.data.mapreduce;
import java.io.IOException;

import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class IrisReducer extends
		Reducer&amp;lt;Text, FloatWritable, Text, FloatWritable&amp;gt; {

	@Override
	public void reduce(Text key, Iterable&amp;lt;FloatWritable&amp;gt; values, Context context)
			throws IOException, InterruptedException {

		float sum = 0;
		float count = 0;
		float mean_sepal_len = 0;
		for (FloatWritable val : values) {
			sum += val.get();
			count++;
		}
		
		mean_sepal_len = sum / count;
		context.write(key, new FloatWritable(mean_sepal_len));
	}

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;run-the-driver&quot;&gt;Run the Driver&lt;/h2&gt;

&lt;p&gt;You should see the following output&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/hadoop/iris_output.png&quot; width=&quot;420&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Header Image - “&lt;strong&gt;Iris&lt;/strong&gt;” by Pauline Rosenberg via &lt;a href=&quot;https://flic.kr/p/6sZzjm&quot;&gt;Flickr&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Sep 2015 15:50:11 -0700</pubDate>
        <link>http://robinsonraju.github.io/tutorial/big-data/2015/09/30/mapreducejob-iris/</link>
        <guid isPermaLink="true">http://robinsonraju.github.io/tutorial/big-data/2015/09/30/mapreducejob-iris/</guid>
        
        
        <category>tutorial</category>
        
        <category>big-data</category>
        
      </item>
    
      <item>
        <title>Writing a simple Mapper and Reducer for Wordcount</title>
        <description>&lt;p&gt;The last blog was about how to run wordcount using the “hadoop-mapreduce-examples.jar” that was automatically available in the VMs. How would it be to really write the Mapper and Reducer for this? I searched a bit for an eclipse plugin to be able to create a MR project through eclipse that would have the template for Mapper and Reducer and I would just write the implementation. Found some plugins (e.g HDT) and some screencasts but didn’t take me far. Remembered that there was an eclipse icon in the Cloudera VM. Started the VM, clicked on the icon and viola ! there was a sample project with Stubs for Mapper, Reducer and Driver. Exactly what I needed!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/hadoop/Stubs.png&quot; width=&quot;420&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Put in the code for Mapper, Reducer and Driver in the Stub classes and ran the Driver. It worked like a charm. Here is the code for it.&lt;/p&gt;

&lt;h2 id=&quot;mapper&quot;&gt;Mapper&lt;/h2&gt;
&lt;p&gt;The map method takes key, value and context as inputs. The key represents the name of a document and the value is the contents of the document. For e.g, a record in a file. The map method below uses StringTokenizer to split the record into words, loops through the words and writes a 2-tuple (word, 1) into the context. Each mapper does this and what we get in the end is a list of key-value pairs.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class StubMapper extends Mapper&amp;lt;LongWritable, Text, Text, IntWritable&amp;gt; {

	private final static IntWritable one = new IntWritable(1);
	private Text word = new Text();

	@Override
	public void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {

		String line = value.toString();
		StringTokenizer tokenizer = new StringTokenizer(line);
		while (tokenizer.hasMoreTokens()) {
			word.set(tokenizer.nextToken());
			context.write(word, one);
		}

	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;reducer&quot;&gt;Reducer&lt;/h2&gt;
&lt;p&gt;A reducer takes an input of a key and list of values associated with the key, adds the counts and writes the output to the context. 
For e.g, input of (world, 1), (world, 1) gives output as (world, 2).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class StubReducer extends
		Reducer&amp;lt;Text, IntWritable, Text, IntWritable&amp;gt; {

	@Override
	public void reduce(Text key, Iterable&amp;lt;IntWritable&amp;gt; values, Context context)
			throws IOException, InterruptedException {

		int sum = 0;
		for (IntWritable val : values) {
			sum += val.get();
		}
		context.write(key, new IntWritable(sum));
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;driver&quot;&gt;Driver&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class StubDriver {

	public static void main(String[] args) throws Exception {

		/*
		 * Validate that two arguments were passed from the command line.
		 */
		if (args.length != 2) {
			System.out.printf(&quot;Usage: StubDriver &amp;lt;input dir&amp;gt; &amp;lt;output dir&amp;gt;\n&quot;);
			System.exit(-1);
		}

		/*
		 * Instantiate a Job object for your job&#39;s configuration.
		 */
		Job job = new Job();

		/*
		 * Specify the jar file that contains your driver, mapper, and reducer.
		 * Hadoop will transfer this jar file to nodes in your cluster running
		 * mapper and reducer tasks.
		 */
		// job.setJarByClass(StubDriver.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);

		job.setMapperClass(StubMapper.class);
		job.setReducerClass(StubReducer.class);

		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		/*
		 * Specify an easily-decipherable name for the job. This job name will
		 * appear in reports and logs.
		 */
		job.setJobName(&quot;Stub Driver&quot;);

		/*
		 * Start the MapReduce job and wait for it to finish. If it finishes
		 * successfully, return 0. If not, return 1.
		 */
		boolean success = job.waitForCompletion(true);
		System.exit(success ? 0 : 1);
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;how-to-run&quot;&gt;How to Run&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;Create an input directory named &quot;input&quot;

Run As &amp;gt; Java Application (give 2 arguments &quot;input&quot; and &quot;output&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Look at the the file ‘part-r-00000’ in the output directory
	&lt;img src=&quot;/img/hadoop/wc-output-eclipse.png&quot; width=&quot;320&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;QuickStart VMs for CDH 5.4.x. (n.d.). Retrieved September 21, 2015, from &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-4-x.html&quot;&gt;http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-4-x.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;WordCount. (n.d.). Retrieved September 21, 2015, from &lt;a href=&quot;https://wiki.apache.org/hadoop/WordCount&quot;&gt;https://wiki.apache.org/hadoop/WordCount&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Header Image - “&lt;strong&gt;Words&lt;/strong&gt;” by Cayce Newell via &lt;a href=&quot;https://flic.kr/p/4BsjLY&quot;&gt;Flickr&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 21 Sep 2015 16:50:11 -0700</pubDate>
        <link>http://robinsonraju.github.io/tutorial/big-data/2015/09/21/simple-map-reduce-job/</link>
        <guid isPermaLink="true">http://robinsonraju.github.io/tutorial/big-data/2015/09/21/simple-map-reduce-job/</guid>
        
        
        <category>tutorial</category>
        
        <category>big-data</category>
        
      </item>
    
      <item>
        <title>Running Hadoop WordCount example</title>
        <description>&lt;p&gt;Here is a small example to run Wordcount program on hadoop. I’ve tried to do this using Cloudera VM and also MapR Sandbox. When you start ClouderaVM, it is already loaded with desktop and you can do all the work inside the desktop - open a browser, start a terminal..etc. MapR Sandbox doesnt come with GUI, so you’d have to ssh into the machine to work on it. You can open &lt;a href=&quot;https://localhost:8443&quot;&gt;https://localhost:8443&lt;/a&gt; in a browser to see Hue to see FileBrowser, DataBrowser, JobBrowser ..etc.&lt;/p&gt;

&lt;h2 id=&quot;using-cloudera-vm&quot;&gt;Using Cloudera VM&lt;/h2&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Download and install &lt;a href=&quot;https://www.virtualbox.org/wiki/Downloads&quot;&gt;&lt;strong&gt;Virtualbox&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Download &lt;a href=&quot;http://cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-4-x.html&quot;&gt;&lt;strong&gt;Cloudera VM&lt;/strong&gt;&lt;/a&gt; and import into VirtualBox.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the cloudera VM, open a terminal and go to the directory that contains hadoop library. 
&lt;code&gt;
cd /usr/lib/hadoop-mapreduce/
&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;create-sample-files&quot;&gt;Create Sample files&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;echo &quot;Hello world in HDFS&quot; &amp;gt; /home/cloudera/testfile1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;echo &quot;Hadoop word count example in HDFS&quot; &amp;gt; /home/cloudera/testfile2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;create-directory-for-the-input-files-on-the-hdfs-file-system-type&quot;&gt;Create directory for the input files on the HDFS file system (type)&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hdfs dfs -mkdir /user/cloudera/input
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;copy-the-files-from-local-filesystem-to-the-hdfs-filesystem&quot;&gt;Copy the files from local filesystem to the HDFS filesystem&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hdfs dfs -put /home/cloudera/testfile1 /user/cloudera/input
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;hdfs dfs -put /home/cloudera/testfile2 /user/cloudera/input
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;run-the-hadoop-wordcount-example&quot;&gt;Run the Hadoop WordCount example&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hadoop jar hadoop-mapreduce-examples.jar wordcount /user/cloudera/input /user/cloudera/output
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;after-completion-view-the-output-directory&quot;&gt;After completion view the output directory&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hdfs dfs -ls /user/cloudera/output
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;check-the-output-file-to-see-the-results&quot;&gt;Check the output file to see the results&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hdfs dfs -cat /user/cloudera/output/part-r-00000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/img/hadoop/wc-output.png&quot; width=&quot;520&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;using-map-r-sandbox&quot;&gt;Using Map-R Sandbox&lt;/h2&gt;

&lt;h3 id=&quot;prerequisites-1&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Download and install &lt;a href=&quot;https://www.virtualbox.org/wiki/Downloads&quot;&gt;&lt;strong&gt;Virtualbox&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Download &lt;a href=&quot;https://www.mapr.com/products/mapr-sandbox-hadoop/download&quot;&gt;&lt;strong&gt;MapR Sandbox&lt;/strong&gt;&lt;/a&gt; and install into VirtualBox.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ssh-into-mapr-sandbox-vm&quot;&gt;ssh into MapR Sandbox VM&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;ssh –p 2222 user01@localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;create-sample-files-1&quot;&gt;Create Sample files&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;mkdir input
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cd input
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;echo &quot;Hello world in HDFS&quot; &amp;gt; testfile1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;echo &quot;Hadoop word count example in HDFS&quot; &amp;gt; testfile2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;run-the-hadoop-wordcount-example-1&quot;&gt;Run the Hadoop WordCount example&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hadoop jar /opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.0-mapr-1506.jar wordcount /user/user01/input /user/user01/output
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;after-completion-view-the-output-directory-1&quot;&gt;After completion, view the output directory&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hadoop fs -ls /user/user01/output
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;check-the-output-file-to-see-the-results-1&quot;&gt;Check the output file to see the results&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hadoop fs -cat /user/user01/output/part-r-00000 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/img/hadoop/wc-output-mapr.png&quot; width=&quot;520&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Header Image - “&lt;strong&gt;n1atsigns2 (Network graph of people on twitter connecting to the topics of Big Data, infochimps or Hadoop)&lt;/strong&gt;” by Philip Kromer via &lt;a href=&quot;https://flic.kr/p/8R7PyB&quot;&gt;Flickr&lt;/a&gt;. Inverterd colors using Photoshop&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 15 Sep 2015 15:50:11 -0700</pubDate>
        <link>http://robinsonraju.github.io/tutorial/big-data/2015/09/15/wordcount-on-hadoop/</link>
        <guid isPermaLink="true">http://robinsonraju.github.io/tutorial/big-data/2015/09/15/wordcount-on-hadoop/</guid>
        
        
        <category>tutorial</category>
        
        <category>big-data</category>
        
      </item>
    
      <item>
        <title>The Checklist Manifesto</title>
        <description>&lt;p&gt;There are many books that when I think back on them, I have this lingering feeling that either due to reading quickly or not taking notes, I didn’t really take in the full essence of the book. There is more to be understood and when there is time, I need to get back to it. “7 Habits of Highly Effective People” by Steven Covey is one such book. My classmate from school, RR, was probably the first person to recommend the book to me. I have probably read the first chapter at least 5 or 6 times before I finally was able to finish reading the book around 2002 when I was working in Chennai. This is the second time I’ve read this book.&lt;/p&gt;

&lt;p&gt;It is one of those books which the moment you finish reading, you will feel inspired, to do more, to yearn to lead a more fulfilling life. You will see the world in a new perspective with a feeling within you that things are more in your control than not.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Habit 1 - Be Proactive&lt;/strong&gt;
This is the ability to control one’s environment, rather than have it control you, as is so often the case. Self determination, choice, and the power to decide response to stimulus, conditions and circumstances&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Habit 2 - begin with the end in mind&lt;/strong&gt;
Covey calls this the habit of personal leadership - leading oneself that is, towards what you consider your aims. By developing the habit of concentrating on relevant activities you will build a platform to avoid distractions and become more productive and successful.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Habit 3 - put first things first&lt;/strong&gt;
Covey calls this the habit of personal management. This is about organising and implementing activities in line with the aims established in habit 2. Covey says that habit 2 is the first, or mental creation; habit 3 is the second, or physical creation. (See the section on time management.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Habit 4 - think win-win&lt;/strong&gt;
Covey calls this the habit of interpersonal leadership, necessary because achievements are largely dependent on co-operative efforts with others. He says that win-win is based on the assumption that there is plenty for everyone, and that success follows a co-operative approach more naturally than the confrontation of win-or-lose.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Habit 5 - seek first to understand and then to be understood&lt;/strong&gt;
One of the great maxims of the modern age. This is Covey’s habit of communication, and it’s extremely powerful. Covey helps to explain this in his simple analogy ‘diagnose before you prescribe’. Simple and effective, and essential for developing and maintaining positive relationships in all aspects of life. (See the associated sections on Empathy, Transactional Analysis, and the Johari Window.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Habit 6 - synergize&lt;/strong&gt;
Covey says this is the habit of creative co-operation - the principle that the whole is greater than the sum of its parts, which implicitly lays down the challenge to see the good and potential in the other person’s contribution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Habit 7 - sharpen the saw&lt;/strong&gt;
This is the habit of self renewal, says Covey, and it necessarily surrounds all the other habits, enabling and encouraging them to happen and grow. Covey interprets the self into four parts: the spiritual, mental, physical and the social/emotional, which all need feeding and developing.&lt;/p&gt;

</description>
        <pubDate>Sun, 06 Sep 2015 01:17:11 -0700</pubDate>
        <link>http://robinsonraju.github.io/books/2015/09/06/The-Checklist-Manifesto/</link>
        <guid isPermaLink="true">http://robinsonraju.github.io/books/2015/09/06/The-Checklist-Manifesto/</guid>
        
        
        <category>books</category>
        
      </item>
    
      <item>
        <title>Data De-Duplication Performance With Multi-Threaded FBC Algorithm</title>
        <description>&lt;p&gt;&lt;strong&gt;Paper reviewed&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Jiang, X., Zhao, J., &amp;amp; Zheng, J. (2014). Enhance Data De – Duplication Performance With Multi – Thread Chunking Algorithm. Santa Clara, CA: Santa Clara University.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We’ve so far reviewed BSW, TTTD and Winnowing algorithms and also reviewed some papers that showed an improvement over TTTD algorithm. 
Through this paper we look at multi-threaded Frequency based Chunking (FBC) method.
FBC makes much improvement in reducing total chunk size and metadata overhead compared to BSW and TTTD but since frequency has to be computed first, it could be slower. 
This paper talks about a multi-threaded FBC that exploits the multicore architecture of the modern microprocessor.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Header Image - “&lt;strong&gt;Columbia Supercomputer&lt;/strong&gt;” by Scott Beale via &lt;a href=&quot;https://flic.kr/p/H8UJY&quot;&gt;Flickr&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 01 Mar 2015 15:13:21 -0800</pubDate>
        <link>http://robinsonraju.github.io/paper/2015/03/01/de-duplication-multi-thread-chunking-algorithm/</link>
        <guid isPermaLink="true">http://robinsonraju.github.io/paper/2015/03/01/de-duplication-multi-thread-chunking-algorithm/</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>Distributional Stability of Chunk Sizes in Data Chunking</title>
        <description>&lt;p&gt;&lt;strong&gt;Paper reviewed&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Zhanjie, W., &amp;amp; Lang, S. (2013). Research on Distributional Stability of Chunk Sizes in Data Chunking. International Journal of Digital Content Technology and Its Applications, 443-450.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Couple of weeks back we’d reviewed TTTD algorithm and an improvement of TTTD. While TTTD can chunk data according to data content and is more efficient than BSW, it has poor stability and the distribution of chunk sizes is too diverging.&lt;/p&gt;

&lt;p&gt;This paper documents research done on distribution of chunk sizes and proposes Two Directions Sliding Window (TDSW) Algorithm for solving the distribution issue.  The paper also talks about experiments with TDSW algorithm which show that there is a reduction of 30% in unbiased variance 
and 50% in running time.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Header Image - “&lt;strong&gt;Axis of stability&lt;/strong&gt;” by TJ Gehling via &lt;a href=&quot;https://flic.kr/p/f2t2ps&quot;&gt;Flickr&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 22 Feb 2015 01:13:21 -0800</pubDate>
        <link>http://robinsonraju.github.io/paper/2015/02/22/distributional-stability-of-chunk-sizes-in-data-chunking/</link>
        <guid isPermaLink="true">http://robinsonraju.github.io/paper/2015/02/22/distributional-stability-of-chunk-sizes-in-data-chunking/</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>Winnowing: local algorithms for document fingerprinting</title>
        <description>&lt;p&gt;&lt;strong&gt;Paper reviewed&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Schleimer, S., Wilkerson, D. S., and Aiken, A. 2003. Winnowing: local algorithms for document fingerprinting. In Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data (San Diego, California, USA, June 09 – 12, 2003).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My focus area right now is data chunking alogrithms. I’d jumped right into data chunking using hashing algorithms since those are popular and there are multiple papers on them. 
This week I thought I’d take a step back and review some other techniques.&lt;/p&gt;

&lt;p&gt;Here is a high level overview of different chunking algorithms -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overlap
    &lt;ul&gt;
      &lt;li&gt;K-gram algorithm&lt;/li&gt;
      &lt;li&gt;0 mod p algorithm&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Winnowing algorithm&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Non-Overlap
    &lt;ul&gt;
      &lt;li&gt;Hash-breaking&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Last paper focused on non-overlap method. This has more information on overlap methods. 
The paper gives an overview of local fingerprinting algorithms. It provides an introduction to Winnowing, a special local fingerprinting algorithm. It goes on to show experimental results for Winnowing’s performance being less than 33% of earlier lower bound.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Header Image - “&lt;strong&gt;Seminole Winnowing Basket&lt;/strong&gt;” by Mathers Museum of World Cultures via &lt;a href=&quot;https://flic.kr/p/pfY1mV&quot;&gt;Flickr&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 14 Feb 2015 17:23:11 -0800</pubDate>
        <link>http://robinsonraju.github.io/paper/2015/02/14/winnowing-local-algorithms-for-doc-fingerprinting/</link>
        <guid isPermaLink="true">http://robinsonraju.github.io/paper/2015/02/14/winnowing-local-algorithms-for-doc-fingerprinting/</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>A Running Time Improvement for TTTD Algorithm</title>
        <description>&lt;p&gt;&lt;strong&gt;Paper reviewed&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Moh, T., &amp;amp; Chang, B. (2010). A Running Time Improvement for the Two Thresholds Two Divisors Algorithm. ACM SE ’10 Proceedings of the 48th Annual Southeast Regional Conference.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Last week I spent some time understanding hash-based data deduplication systems. It felt great to revisit Rabin-Karp Algorithm for string matching. This algorithm was my focus in CS 255 and I had implemented it using Monte Carlo and Las Vegas approaches (A Monte Carlo algorithm has a guaranteed completion time but fails to output a correct answer with a small probability, A Las Vegas algorithm has guaranteed correctness but fails to complete with a small probability). 
Before trying to dedupe data, it needs to be chunked. There are many algorithms to do that. The Basic Sliding Window (BSW) algorithm is one of the earliest and widely used. The Two Thresholds Two Divisors (TTTD) algorithm is an improvement over this. 
This paper proposes an improvement over the TTTD algorithm.&lt;/p&gt;

&lt;p&gt;I still need to find a better system to embed a presentation in a static page. For now using issuu.&lt;/p&gt;

&lt;div data-configid=&quot;18778196/14985334&quot; style=&quot;width:625px; height:500px;&quot; class=&quot;issuuembed&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//e.issuu.com/embed.js&quot; async=&quot;true&quot;&gt;&lt;/script&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Header Image - “&lt;strong&gt;Curiousity&lt;/strong&gt;” by Thomas Hawk via &lt;a href=&quot;https://flic.kr/p/dSuxV1&quot;&gt;Flickr&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 08 Feb 2015 03:33:21 -0800</pubDate>
        <link>http://robinsonraju.github.io/paper/2015/02/08/running-time-improvement-for-tttd-algorithm/</link>
        <guid isPermaLink="true">http://robinsonraju.github.io/paper/2015/02/08/running-time-improvement-for-tttd-algorithm/</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>Let sleeping files lie: Pattern matching in Z-compressed file</title>
        <description>&lt;p&gt;This is a review of the paper on Pattern matching in Z-compressed files. I’d done a project last year to implement LZW compression and chanced upon this paper while I was googling for some information on LZW. To see two areas that I’d worked on in the past - Data Search (Had helped build a search engine using Lucene 10 yrs back and used Solr for another project a few years back) and Data Compression come together, to know that we would be able to index data and search from compressed files without decompressing them, was very intersting and I wanted to find more information and see some implementation of this.&lt;br /&gt;
The review of the paper was presented to some students at SJSU.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Paper reviewed&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Amir, A., Benson, G., &amp;amp; Farach, M. (1996). Let sleeping files lie: Pattern matching in Z-compressed file. Journal of System Sciences, 52, 299-307.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The presetation had the following sections -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overview of Data compression&lt;/li&gt;
  &lt;li&gt;Details about LZW compression&lt;/li&gt;
  &lt;li&gt;Compressed Pattern Matching&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;TBD&lt;/strong&gt; Hope to put the ppt here (once I learn to use reveal.js here!)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;[1]  A. Amir, G. Benson and M. Farach, “Let sleeping files lie: Pattern matching in Z-compressed file”, Journal of System Sciences, 52: 299-307, 1996.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[2] M. Farach and M. Thorup, “String matching in Lempel-Ziv compressed strings”, Algorithmica, 20: 388-404, 1998&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[3] D. E. Knuth, J. H. Morris V. R. Pratt, SIAM J. Comput. 6, 323, 1977&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[4] G. Navarro and M. Raffinot, “A general practical approach to pattern matching over Ziv-Lempel compressed text”, Proceedings, Combinatorial Pattern Matching, LNCS 1645: pp. 14-36, 1999.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[5] N. Zhang, T. Tao, R. V. Satya and A. Mukherjee, “A modified LZW algorithm for compressed text retrieval with random access property”, draft, Computer Science Department, University of Central Florida, 2004.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Header Image - “&lt;strong&gt;Elephant seals asleep&lt;/strong&gt;” by Bill Abbott via &lt;a href=&quot;https://flic.kr/p/2bVDZF&quot;&gt;Flickr&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 31 Jan 2015 20:35:12 -0800</pubDate>
        <link>http://robinsonraju.github.io/paper/2015/01/31/compressed-pattern-matching/</link>
        <guid isPermaLink="true">http://robinsonraju.github.io/paper/2015/01/31/compressed-pattern-matching/</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>The End of Power</title>
        <description>&lt;p&gt;&lt;em&gt;Mark Zuckerberg posts a public challenge/goal for himself at the beginning of every year and in 2015 it was to &lt;a href=&quot;https://www.facebook.com/ayearofbooks/&quot;&gt;read a new book every two weeks&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Power no longer buys as much as it did in the past. In the 21st century, power is easy to get, harder to use - and easy to lose.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://www.amazon.com/End-Power-Boardrooms-Battlefields-Churches/dp/0465065694&quot;&gt;‘The End of Power’&lt;/a&gt; was the first book chosen. The theme of the book is that power is not what it used to be. People in power are not as powerful as in the past and not as much as we sometimes think them to be. Power has weakened because barriers to power have reduced by the &lt;strong&gt;“More”, “Mobility”&lt;/strong&gt; and &lt;strong&gt;“Mentality”&lt;/strong&gt; revolution. The book makes one think about the world around us. In a way, the world has become more democratic and ordinary people have access/chance to rise to the top but when one looks at the world from the top, it is more chaotic and difficult to manage. One might choose to not get into management/leadership positions because of this.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;chapter-1-the-decay-of-power&quot;&gt;Chapter 1	: The Decay of Power&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Power is the ability to direct or prevent the current or future actions of other groups and individuals.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;This chapter gives the example of James Black Jr., a 12 yr old from a working class family in Brooklyn, NY who becomes a Chess Grandmaster at the age of 12. It is a global trend. More players are learning the game and achieving mastery sooner than their predecesors, many of them on their own!. The reason is because the easier access to knowledge - of millions of games by other Grandmasters or computer simulation of moves, made by digital revolution and the internet. The author goes on to say that the similar trend is seen in all the other aspects of life. The chapter also gives us an overview of what the book’s is going to talk about.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;chapter-2-making-sense-of-power-how-it-works-and-how-to-keep-it&quot;&gt;Chapter 2	: Making Sense of Power: How It Works and How to Keep It&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Identify the barriers to power and whether they are coming up or going down, and you can solve a large part of the puzzle of power.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;The main theme of this chapter is extend the definition of power in the first chapter. Power is expressed through four different means - The Muscle, The Code, The Pitch, The Reward.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;MacMillan’s Taxonomy of Power&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Outcome seen as improvement&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Outcome seen as nonimprovement&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Change incentives&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Inducement via reward: Increase the salary, lower a price&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Coercion via muscle: Law enforcement, repression, violence&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Change preferences&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Persuasion via pitch: Advertising, campaigning&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Obligation via code: Religious or traditional duty, moral suasion&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;chapter-3-how-power-got-big-an-assumptions-unquestioned-rise&quot;&gt;Chapter 3	: How Power Got Big: An Assumption’s Unquestioned Rise&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Weber’s central message was that without a reliable, well-functioning organization, or, to use his term, without a bureaucracy, power could not be effectively wielded.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;The main theme of this chapter is to about how companies throughout the world became huge, especially after the World War II. Huge companies could be managed only via hierarchies and bureaucracy. The author talks in depth about Max Weber, German sociologist who analyzed Western economies and wrote about bureaucracy and how it would lead to power being managed by big corporations.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;chapter-4-how-power-lost-its-edge-the-more-mobility-and-mentality-revolutions&quot;&gt;Chapter 4	: How Power Lost Its Edge: The More, Mobility, and Mentality Revolutions&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;“The decoupling of power from size, and thus the decoupling of the capacity to use power effectively from the control of a large Weberian bureaucracy, is changing the world.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;The &lt;strong&gt;More&lt;/strong&gt; revolution - Overwhelming the means of control&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The &lt;strong&gt;Mobility&lt;/strong&gt; revolution - The end of captive audiences&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The &lt;strong&gt;Mentality&lt;/strong&gt; revolution - Taking nothing for granted anymore&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Power and the three revolutions:&lt;/strong&gt;&lt;/em&gt;
&lt;img src=&quot;/img/books/EndOfPower-ch4.png&quot; alt=&quot;alt text&quot; title=&quot;Power and the 3 revolutions&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;chapter-5-why-are-landslides-majorities-and-mandates-endangered-species-the-decay-of-power-in-national-politics&quot;&gt;Chapter 5	: Why Are Landslides, Majorities, and Mandates Endangered Species? The Decay of Power in National Politics&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Accordingly, the noble art of governing now depends more heavily on a much dirtier, hands-on skill: forming and maintaining a coalition.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;The main theme of this chapter is the before and after contrast of the governments across the world - number of sovereign nations has quadrupled since 1945, democracies have increased, autocracies have gone down, there are more hung parliaments now, very few landslide victories, etc.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;chapter-6-pentagons-versus-pirates-the-decaying-power-of-large-armies&quot;&gt;Chapter 6	: Pentagons Versus Pirates: The Decaying Power of Large Armies&lt;/h4&gt;

&lt;h4 id=&quot;chapter-7-whose-world-will-it-be-vetoes-resistance-and-leaks-or-why-geopolitics-is-turning-upside-down&quot;&gt;Chapter 7	: Whose World Will It Be? Vetoes, Resistance, and Leaks-or Why Geopolitics Is Turning Upside Down&lt;/h4&gt;

&lt;h4 id=&quot;chapter-8-business-as-unusual-corporate-dominance-under-siege&quot;&gt;Chapter 8	: Business as Unusual: Corporate Dominance Under Siege&lt;/h4&gt;

&lt;h4 id=&quot;chapter-9-hyper-competition-for-your-soul-heart-and-brain&quot;&gt;Chapter 9	: Hyper-Competition for Your Soul, Heart, and Brain&lt;/h4&gt;

&lt;h4 id=&quot;chapter-10--the-decay-of-power-is-the-glass-half-full-or-half-empty&quot;&gt;Chapter 10 : The Decay of Power: Is the Glass Half-Full or Half-Empty?&lt;/h4&gt;

&lt;h4 id=&quot;chapter-11--power-is-decaying-so-what-what-to-do&quot;&gt;Chapter 11 : Power Is Decaying: So What? What to Do?&lt;/h4&gt;

</description>
        <pubDate>Sat, 10 Jan 2015 02:17:11 -0800</pubDate>
        <link>http://robinsonraju.github.io/books/2015/01/10/End-of-Power/</link>
        <guid isPermaLink="true">http://robinsonraju.github.io/books/2015/01/10/End-of-Power/</guid>
        
        
        <category>books</category>
        
      </item>
    
  </channel>
</rss>
